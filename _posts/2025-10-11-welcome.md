---
title: ML Foundamentals - Part 1
date: 2025-10-11 10:50
categories: [ML]
tags: [Neuron, Dot Product, Activation Functions, Loss]
author: James Huang
---

## Introduction
In this post, we’ll peel back the curtain on the core building blocks of neural networks. 

- **Neuron**: how a single neuron (or “node”) aggregates inputs and produces an output  
- **Dot Product**: the mathematical engine that lets inputs and weights interact  
- **Activation Functions**: how we inject non‑linearity so our networks can learn complex patterns  
- **Loss**: how we measure how well (or poorly) our model is doing, and how that drives learning  

## Neurons

A **neuron** (also called a “node” or “unit”) is the fundamental building block of a neural network. Here’s how it works, in simple terms:


### Inputs and Weights

A **neuron** takes in inputs, applies weights, adds a bias, and produces an output. Let’s break this down:

- A neuron receives **inputs** — these could be features from your data, like (x₁, x₂, x₃, …).
- Each input has a **weight** — (w₁, w₂, w₃, …) — telling the neuron how important that input is.
- The neuron multiplies each input by its weight and adds them all together.
- We usually also add a **bias** (b), which helps shift the result.

**Mathematically:**
\[
z = (w_1 \cdot x_1) + (w_2 \cdot x_2) + \cdots + (w_n \cdot x_n) + b
\]

Or in summation notation:

\[
z = \sum_{i=1}^{n} w_i x_i + b
\]

### 3. Why Neurons Are Useful

- Neurons let us model **complex relationships** by combining input features in flexible ways.  
- By stacking many neurons into **layers**, a neural network can learn hierarchical patterns (e.g. edges → shapes → objects).  
- During training, weights and biases are adjusted so that the neuron outputs match desired outputs (this involves dot products, activation, and loss — coming next).
